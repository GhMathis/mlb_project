---
title: "Prédiction de la richesse spécifique tibétaine"
author: "Quentin Lamboley, Jeanne Thill, Mathis Gueno & Lucien Ricome"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r}
library(readxl)
library(tidyverse)
library(glmnet)
library(randomForest)
library(caret)
library(doParallel)
library(gbm)
library(kernlab)
library(e1071)
library(ggforce)
library(keras)
library(tensorflow)


```

# Importation des données

On commence par importer les données. On nomme les variable du data frame et on lui retire la première colonne qui contient les indices des lignes.

```{r data.import}

Tibet = read.table("data.txt",header=TRUE,sep="\t",dec = ",")


Tibet<-as.data.frame(Tibet)
names(Tibet) = c("ID","A_Temp","Sp_temp","Su_temp","W_temp","A_prec","Sp_prec",
                 "Su_prec","W_prec","A_rad","Sp_rad","Su_rad","W_rad","Silt",
                 "Sand","Clay","Aspect","DEM","Slope","pH","SOM","SR")
Tibet<-Tibet[,-1]
```

# Création des fonction pour chaque modèle

Les fonctions permettent de clarifier les chunks itératifs.

## Forêt aléatoire

On utilise ici la fonction "randomForest" du package randomForest (version 4.7 - 1.1) qui permet de calculer les prédictions et l'importance des variables avec la forêt aléatoire. 
Avec comme arguments :
* mtry = le nombre de variable sélectionnées pour construire chaque arbre; 
* ntree = le nombre d'arbre généré par l'algorithme

```{r Foret Alea}
ForetAlea <- function(Tibetapp,Tibetvalid,Results){
  
  #selections du nb de variables utiliser pour le modele = mtry ?

  
  # entrainement du model sur le jeu d'entrainement : Tibetapp
  
  ForeTibet<-randomForest(SR~.,data=Tibetapp,mtry=(ncol(Tibetapp)-1)/3,ntree=250,keep.forest=TRUE)
  
  # Prediction sur le jeu de validation : Tibetvalid
  
  prev<-predict(ForeTibet,newdata=Tibetvalid[,1:20])
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSERandomF[i]<-sqrt(mean((Tibetvalid[,21]-prev)^2))
  
  # Stockages des variables utilisées pour le modele
  
  return(Results)
}

#Regression sous contraintes
```

## Régression avec pénalité Lasso

On utilise ici la fonction "cv.glmet" du package glmnet (version 4.1 - 8). Les arguments sont :
* Une matrice des variables explicatives
* Un vecteur de la variable réponse
* type.mesure = méthode utilisée pour la validation croisée.

```{r Lasso}
Lasso <- function(Tibetapp, Tibetvalid, Results){
  
  # entrainement du modele sur le jeu d'entrainement : Tibetapp
  lasso<-cv.glmnet(as.matrix(Tibetapp[,1:20]),Tibetapp[,21],type.measure="mse")
  
  # Prediction sur le jeu de validation : Tibetvalid 
  prev.lasso<-predict(lasso,newx=as.matrix(Tibetvalid[,1:20]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSERegLasso[i]<-sqrt(mean((Tibetvalid[,21]-prev.lasso)^2))
  
  #Stockage des coefficient associer à chaque variable ?
  
  return(Results)
}
```


## Régression Ridge


On utilise ici 
```{r Ridge}
Ridge <- function(Tibetapp, Tibetvalid, Results){
  
  # entrainement du modele sur le jeu d'entrainement : Tibetapp
  ridge<-cv.glmnet(as.matrix(Tibetapp[,1:20]),Tibetapp[,21],alpha=0,type.measure="mse")
  
  # Prediction sur le jeu de validation : Tibetvalid 
  prev.ridge<-predict(ridge,newx=as.matrix(Tibetvalid[,1:20]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSERegRidge[i]<-sqrt(mean((Tibetvalid[,21]-prev.ridge)^2))
  
  #Stockage des coefficient associer à chaque variable ?
  
  return(Results)
}
```

## Régression Elasticnet

```{r Elast}
Elasticnet <- function(Tibetapp, Tibetvalid, Results){
  
  # entrainement du modele sur le jeu d'entrainement : Tibetapp
  elasticnet<-cv.glmnet(as.matrix(Tibetapp[,1:20]),Tibetapp[,21],alpha=0.5,type.measure="mse")
  
  # Prediction sur le jeu de validation : Tibetvalid 
  prev.elasticnet<-predict(elasticnet,newx=as.matrix(Tibetvalid[,1:20]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSERegelasticnet[i]<-sqrt(mean((Tibetvalid[,21]-prev.elasticnet)^2))
  
  #Stockage des coefficient associer à chaque variable ?
  
  return(Results)
}
```

## Gradient boosting

```{r GradB}
GradientBoosting <- function(Tibetapp, Tibetvalid, Results){
  # entrainement du modele sur le jeu d'entrainement : Tibetapp
  Gradboost<-gbm(SR~.,data=Tibetapp,distribution = 'gaussian',shrinkage=0.01,n.trees=3000) 
  
  # Prediction sur le jeu de validation : Tibetvalid 
  prev.GB<-predict(Gradboost,newdata=Tibetvalid[,1:20])
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSEGB[i]<-sqrt(mean((Tibetvalid[,21]-prev.GB)^2))
  
  #Stockage des coefficient associer à chaque variable ?
  
  return(Results)
}
```

## Régession SVM

```{r SVM}
svm.regression = function(Tibetapp, Tibetvalid, Results){
  
  # entrainement du modele sur le jeu d'entrainement : Tibetapp
  Modsvm = svm(SR~.,data=Tibetapp)
  
  # Prediction sur le jeu de validation : Tibetvalid 
  prev.svm<-predict(Modsvm,newdata=Tibetvalid[,1:20])
  
  # Calcul du RMSE pour évaluer la qualité du modele
  Results$RMSESVM[i]<-sqrt(mean((Tibetvalid[,21]-prev.svm)^2))
  
  return(Results)
}
```


# Lancement des modèles et récupération des résultats

## Création du tableau des sorties

```{r}
Results<-matrix(0,ncol=8,nrow=1000,dimnames=list(1:1000,c("RMSERandomF","RMSERegLasso","RMSERegRidge","RMSERegelasticnet","RMSEGB","RMSESVM","RMSEResNeu","BestMod")))
Results<-as.data.frame(Results)

```

## Itérations des algorithmes et stockage des résultats

### Forêt aléatoire

```{r Loop.ForetAlea}
set.seed(100010)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)
if(file.exists("RMSERandomF.txt")){
  Resultforet = read.table("RMSERandomF.txt")
  Results$RMSERandomF = Resultforet$x
}else{
  for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
    
    #Sélection du jeu de données-Validation non croisée de Monte-Carlo
    aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
    #Séparation en un jeu de données en un d'apprentissage et un de validation
    Tibetapp<-Tibet[aleat,]
    Tibetvalid<-Tibet[-aleat,]
    ##############################################################################
    #Rajouter transformation des variables?Regression de poisson?package keras?
    
    #Foret Aléatoire 
    Results <- ForetAlea(Tibetapp,Tibetvalid,Results)
  
  }
}
stopCluster(cl)
```

### Régression Lasso

```{r Loop.Lasso}
set.seed(100010)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)
if(file.exists("Resultreg.txt")){
  Resultreg = read.table("Resultreg.txt")
  Results$RMSERegLasso = Resultreg$RMSERegLasso
  Results$RMSERegRidge = Resultreg$RMSERegRidge
  Results$RMSERegelasticnet = Resultreg$RMSERegelasticnet
}else{
  for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
    
    #Sélection du jeu de données-Validation non croisée de Monte-Carlo
    aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
    #Séparation en un jeu de données en un d'apprentissage et un de validation
    Tibetapp<-Tibet[aleat,]
    Tibetvalid<-Tibet[-aleat,]
    ##############################################################################
    #Rajouter transformation des variables?Regression de poisson?package keras?
    
    # Lasso 
    Results <- Lasso(Tibetapp,Tibetvalid,Results)
    
  }
  
  for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
    
    #Sélection du jeu de données-Validation non croisée de Monte-Carlo
    aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
    #Séparation en un jeu de données en un d'apprentissage et un de validation
    Tibetapp<-Tibet[aleat,]
    Tibetvalid<-Tibet[-aleat,]
    ##############################################################################
    #Rajouter transformation des variables?Regression de poisson?package keras?
    
    # Ridge 
    Results <- Ridge(Tibetapp,Tibetvalid,Results)
  
  }
  

  for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
    
    #Sélection du jeu de données-Validation non croisée de Monte-Carlo
    aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
    #Séparation en un jeu de données en un d'apprentissage et un de validation
    Tibetapp<-Tibet[aleat,]
    Tibetvalid<-Tibet[-aleat,]
    ##############################################################################
    #Rajouter transformation des variables?Regression de poisson?package keras?
    
    # Elasticnet 
    Results <- Elasticnet(Tibetapp,Tibetvalid,Results)
  
  }
}
stopCluster(cl)
```

### Gradient boosting

```{r Loop.GradB}
set.seed(100010)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)
for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
  
  #Sélection du jeu de données-Validation non croisée de Monte-Carlo
  aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
  #Séparation en un jeu de données en un d'apprentissage et un de validation
  Tibetapp<-Tibet[aleat,]
  Tibetvalid<-Tibet[-aleat,]
  ##############################################################################
  #Rajouter transformation des variables?Regression de poisson?package keras?
  
  #Gradient boosting 
  Results <- GradientBoosting(Tibetapp,Tibetvalid,Results)
}
stopCluster(cl)
```

### SVM

```{r Loop.SVM}
set.seed(100010)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)
for (i in 1:1000){ # Faire des boucles séparées pour chaque methode en vérifant avant le temps que ça prend pour tourner une seule fois 
  
  #Sélection du jeu de données-Validation non croisée de Monte-Carlo
  aleat<-sample(length(Tibet$A_Temp),250) #explication du 250 à faire
  #Séparation en un jeu de données en un d'apprentissage et un de validation
  Tibetapp<-Tibet[aleat,]
  Tibetvalid<-Tibet[-aleat,]
  ##############################################################################
  #Support vector machine (mode regression) 
  Results <- svm.regression(Tibetapp,Tibetvalid,Results)
}
stopCluster(cl)
```



```{r}
Rsq <- function(RMSE){
  SSE <- length(Tibetvalid[,21])*RMSE^2
  SST = sum((Tibetvalid[,21]-mean(Tibetvalid[,21]))^2)
  Rsq = 1 - SSE/SST
  return(Rsq)
}
```


# Resultats

```{r RMSE.plot}
Results_long = Results%>%
  select(starts_with("RMSE"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("Grad.boost" ,"Random.F", "Elsast", "Lasso", "Ridge", "ResNeu", "SVM")
recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.rmse = mean(value),
            sd.rmse = sd(value))
ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.rmse), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.rmse-sd.rmse, ymax=mean.rmse+sd.rmse),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  labs(y= "Erreur moyenne",x ="Modèles")+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

```

# Annexe

Justification du choix de travailler avec une loi normale et non une loi de poisson

```{r}
# Génération des valeurs pour la distribution de Poisson
lambda = mean(Tibet$SR)
x <- 0:30  
poisson_values <- dpois(x, lambda) 

# Génération des valeurs pour la distribution normale
norm_values <- dnorm(x,mean(Tibet$SR),sd(Tibet$SR))

# Création du graphique
par(mfrow = c(1,3))

plot(x, poisson_values, type = "h", lwd = 2, col = "blue",
     xlab = "Richesse specifique", ylab = "Probabilité",
     main = "Distribution de la loi de Poisson")

plot(x, norm_values, type = "h", lwd = 2, col = "blue",
     xlab = "Richesse specifique", ylab = "Probabilité",
     main = "Distribution de la loi normale")

hist(Tibet$SR,xlab = "Richesse specifique",main = "Distribution des données")
```

QQ plot des données de richesse spécifique

```{r}
qqnorm(Tibet$SR, pch= 16, col = 'blue',xlab='')
qqline(Tibet$SR, col= 'red')
```


